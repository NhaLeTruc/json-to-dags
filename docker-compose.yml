# ============================================================================
# Apache Airflow ETL Demo Platform - Docker Compose Configuration
# ============================================================================
# Services: Airflow (webserver, scheduler, init), PostgreSQL (Airflow + Warehouse)
# Network: Internal bridge network for service communication
# ============================================================================

x-airflow-common:
  &airflow-common
  # Migrated from Airflow 3.0 to 2.10.5 for stability
  # Using official image instead of custom build
  image: apache/airflow:2.10.5-python3.12
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    # WARNING: Empty Fernet key is INSECURE - generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
    # For production, set AIRFLOW__CORE__FERNET_KEY in .env file or use secrets management
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY:-}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # Removed Airflow 3.0 specific configs (TASK_RUNNER, MP_START_METHOD)
    PYTHONPATH: /opt/airflow:/opt/airflow/dags:/opt/airflow/src
    # Warehouse connection environment variables
    WAREHOUSE_HOST: ${WAREHOUSE_HOST:-airflow-warehouse}
    WAREHOUSE_PORT: ${WAREHOUSE_PORT:-5432}
    WAREHOUSE_DB: ${WAREHOUSE_DB:-warehouse}
    WAREHOUSE_USER: ${WAREHOUSE_USER:-warehouse_user}
    WAREHOUSE_PASSWORD: ${WAREHOUSE_PASSWORD:-warehouse_pass}
    # Airflow connection for warehouse (programmatic access)
    # Connection ID "warehouse" matches the postgres_conn_id used in DAGs
    AIRFLOW_CONN_WAREHOUSE: postgresql://${WAREHOUSE_USER:-warehouse_user}:${WAREHOUSE_PASSWORD:-warehouse_pass}@${WAREHOUSE_HOST:-airflow-warehouse}:${WAREHOUSE_PORT:-5432}/${WAREHOUSE_DB:-warehouse}
    SMTP_HOST: ${SMTP_HOST:-}
    SMTP_PORT: ${SMTP_PORT:-587}
    SMTP_USER: ${SMTP_USER:-}
    SMTP_PASSWORD: ${SMTP_PASSWORD:-}
    SMTP_FROM: ${SMTP_FROM:-}
    TEAMS_WEBHOOK_URL: ${TEAMS_WEBHOOK_URL:-}
    TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN:-}
    TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID:-}
    # Install additional providers
    _PIP_ADDITIONAL_REQUIREMENTS: apache-airflow-providers-postgres==5.13.1
  volumes:
    - ./dags:/opt/airflow/dags
    - ./src:/opt/airflow/src
    - ./tests:/opt/airflow/tests
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./data:/opt/airflow/data
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    airflow-postgres:
      condition: service_healthy
    airflow-warehouse:
      condition: service_healthy

services:
  # ============================================================================
  # PostgreSQL - Airflow Metadata Database
  # ============================================================================
  airflow-postgres:
    image: postgres:15-alpine
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - airflow-network

  # ============================================================================
  # PostgreSQL - Mock Data Warehouse
  # ============================================================================
  airflow-warehouse:
    build:
      context: .
      dockerfile: docker/warehouse/Dockerfile
    container_name: airflow-warehouse
    environment:
      POSTGRES_USER: warehouse_user
      POSTGRES_PASSWORD: warehouse_pass
      POSTGRES_DB: warehouse
    volumes:
      - airflow-warehouse-data:/var/lib/postgresql/data
      - ./src/warehouse/migrations:/migrations
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "warehouse_user", "-d", "warehouse"]
      interval: 10s
      retries: 5
      start_period: 10s
    restart: always
    networks:
      - airflow-network

  # ============================================================================
  # Airflow - Database Initialization
  # ============================================================================
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate
        airflow users create \
          --username "${_AIRFLOW_WWW_USER_USERNAME:-admin}" \
          --password "${_AIRFLOW_WWW_USER_PASSWORD:-admin}" \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com
        # Create filesystem connection for FileSensor (used by event-driven DAGs)
        airflow connections add 'fs_default' \
          --conn-type 'fs' \
          --conn-extra '{"path": "/"}' \
          2>/dev/null || echo "fs_default connection already exists"
        # Create Spark connections for multi-cluster demo DAG
        # These are placeholder connections - actual Spark clusters not included in demo
        airflow connections add 'spark_standalone' \
          --conn-type 'spark' \
          --conn-host 'spark-master' \
          --conn-port 7077 \
          --conn-extra '{"spark_binary": "spark-submit", "deploy_mode": "client"}' \
          2>/dev/null || echo "spark_standalone connection already exists"
        airflow connections add 'spark_yarn' \
          --conn-type 'spark' \
          --conn-host 'yarn-resourcemanager' \
          --conn-extra '{"spark_binary": "spark-submit", "deploy_mode": "cluster", "queue": "default"}' \
          2>/dev/null || echo "spark_yarn connection already exists"
        airflow connections add 'spark_k8s' \
          --conn-type 'spark' \
          --conn-host 'kubernetes.default.svc' \
          --conn-extra '{"spark_binary": "spark-submit", "namespace": "spark-jobs"}' \
          2>/dev/null || echo "spark_k8s connection already exists"
        echo "Airflow initialization complete"
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-admin}
    user: "airflow:0"
    depends_on:
      <<: *airflow-common-depends-on
    networks:
      - airflow-network

  # ============================================================================
  # Airflow - Database Schema Fixes
  # ============================================================================
  airflow-db-fix:
    image: postgres:15-alpine
    container_name: airflow-db-fix
    environment:
      PGHOST: ${POSTGRES_AIRFLOW_HOST:-airflow-postgres}
      PGPORT: ${POSTGRES_AIRFLOW_PORT:-5432}
      PGDATABASE: ${POSTGRES_AIRFLOW_DB:-airflow}
      PGUSER: ${POSTGRES_AIRFLOW_USER:-airflow}
      PGPASSWORD: ${POSTGRES_AIRFLOW_PASSWORD:-airflow}
    command:
      - sh
      - -c
      - |
        echo "Waiting for Airflow initialization to complete..."
        sleep 5
        echo "Applying database schema fixes..."
        psql -f /docker/airflow/fix_callback_type.sql
        echo "Database schema fixes applied successfully"
    volumes:
      - ./docker:/docker:ro
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - airflow-network

  # ============================================================================
  # Airflow - Web Server
  # ============================================================================
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks:
      - airflow-network

  # ============================================================================
  # Airflow - Scheduler
  # ============================================================================
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-db-fix:
        condition: service_completed_successfully
      airflow-webserver:
        condition: service_healthy
    environment:
      <<: *airflow-common-env
      # Scheduler robustness settings
      AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: 5
      AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 512
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
      AIRFLOW__SCHEDULER__ZOMBIE_TASK_TIMEOUT: 300
      AIRFLOW__CORE__PARALLELISM: 32
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 16
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    networks:
      - airflow-network

# ============================================================================
# Volumes
# ============================================================================
volumes:
  airflow-postgres-data:
    name: airflow-postgres-data
  airflow-warehouse-data:
    name: airflow-warehouse-data

# ============================================================================
# Networks
# ============================================================================
networks:
  airflow-network:
    driver: bridge
    name: airflow-network
